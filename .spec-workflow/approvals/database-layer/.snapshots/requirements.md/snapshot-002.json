{
  "id": "snapshot_1764071382837_qzc2rowyy",
  "approvalId": "approval_1764071288263_9wel0xdo5",
  "approvalTitle": "Database Layer Requirements Document",
  "version": 2,
  "timestamp": "2025-11-25T11:49:42.837Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Requirements Document: Database Layer\n\n## Introduction\n\nThe Database Layer is the CardLink component responsible for persistent storage of device configurations, UICC card profiles, OTA session records, communication logs, test results, and server settings. It provides a unified data access layer using SQLAlchemy ORM with support for SQLite (default), MySQL, and PostgreSQL backends.\n\nThis component enables CardLink to maintain state across sessions, track test history, and support multi-user deployments with shared database backends.\n\n## Alignment with Product Vision\n\nThis feature directly supports CardLink's core mission of providing accessible SCP81 compliance testing:\n\n- **Configuration persistence**: Store device profiles, card configurations, and server settings\n- **Session history**: Track OTA sessions for analysis and debugging\n- **Communication logs**: Persist APDU exchanges for protocol analysis\n- **Test results**: Store test execution history for compliance reporting\n- **Multi-backend support**: SQLite for single-user, MySQL/PostgreSQL for teams\n\n## Requirements\n\n### Requirement 1: Database Connection Management\n\n**User Story:** As a developer, I want the database layer to manage connections automatically, so that I can focus on application logic without worrying about connection lifecycle.\n\n#### Acceptance Criteria\n\n1. WHEN the application starts THEN the database layer SHALL parse DATABASE_URL environment variable\n2. WHEN DATABASE_URL is not set THEN the layer SHALL default to SQLite at `data/cardlink.db`\n3. WHEN connecting to SQLite THEN the layer SHALL create the database file if it doesn't exist\n4. WHEN connecting to MySQL/PostgreSQL THEN the layer SHALL validate connection parameters\n5. WHEN connection fails THEN the layer SHALL provide clear error message with troubleshooting hints\n6. WHEN using connection pool THEN the layer SHALL configure appropriate pool size per backend\n7. WHEN the application shuts down THEN the layer SHALL close all connections gracefully\n\n### Requirement 2: Schema Management and Migrations\n\n**User Story:** As a developer, I want database schema changes to be managed through migrations, so that I can safely upgrade production databases without data loss.\n\n#### Acceptance Criteria\n\n1. WHEN initializing database THEN the layer SHALL create all required tables\n2. WHEN schema changes are needed THEN developers SHALL create Alembic migrations\n3. WHEN running migrations THEN the layer SHALL apply changes in order\n4. WHEN downgrading THEN the layer SHALL support migration rollback\n5. WHEN checking migration status THEN the layer SHALL report current version\n6. WHEN migrations fail THEN the layer SHALL rollback transaction and report error\n7. WHEN using SQLite THEN the layer SHALL handle ALTER TABLE limitations\n\n### Requirement 3: Device Configuration Storage\n\n**User Story:** As a tester, I want to save and retrieve device configurations, so that I can quickly reconnect to previously configured phones and modems.\n\n#### Acceptance Criteria\n\n##### Phone Device Configuration\n1. WHEN saving phone config THEN the layer SHALL store:\n   - Device ID (ADB serial)\n   - Device name/alias\n   - Manufacturer, model, Android version\n   - IMEI, IMSI, ICCID\n   - Connection settings (USB port, etc.)\n   - Last seen timestamp\n   - Custom notes\n\n##### Modem Device Configuration\n2. WHEN saving modem config THEN the layer SHALL store:\n   - Device ID (serial port identifier)\n   - Device name/alias\n   - Manufacturer, model, firmware version\n   - IMEI, IMSI, ICCID\n   - Serial port settings (baud rate, etc.)\n   - AT command preferences\n   - Last seen timestamp\n\n##### Common Operations\n3. WHEN listing devices THEN the layer SHALL support filtering by type, status, last seen\n4. WHEN updating device THEN the layer SHALL preserve creation timestamp\n5. WHEN deleting device THEN the layer SHALL optionally cascade to related sessions\n6. WHEN searching devices THEN the layer SHALL support partial matching on name/ID\n\n### Requirement 4: UICC Card Profile Storage\n\n**User Story:** As a tester, I want to store UICC card profiles in the database, so that I can manage multiple test cards and their configurations.\n\n#### Acceptance Criteria\n\n1. WHEN saving card profile THEN the layer SHALL store:\n   - ICCID (primary identifier)\n   - IMSI\n   - Card type (UICC, USIM, eUICC)\n   - ATR\n   - PSK identity\n   - PSK key (encrypted)\n   - Admin server URL\n   - Trigger configuration (JSON)\n   - BIP configuration (JSON)\n   - Security Domain info\n   - Creation/modification timestamps\n   - Custom notes\n\n2. WHEN storing PSK key THEN the layer SHALL encrypt using application secret\n3. WHEN listing profiles THEN the layer SHALL support filtering by card type, PSK status\n4. WHEN exporting profile THEN the layer SHALL support JSON format with optional key inclusion\n5. WHEN importing profile THEN the layer SHALL validate and detect conflicts by ICCID\n6. WHEN associating card with device THEN the layer SHALL track which device used which card\n\n### Requirement 5: OTA Session Storage\n\n**User Story:** As a tester, I want OTA sessions to be recorded in the database, so that I can review session history and analyze patterns.\n\n#### Acceptance Criteria\n\n1. WHEN creating session THEN the layer SHALL store:\n   - Session ID (UUID)\n   - Device ID (phone or modem)\n   - Card ICCID\n   - Session type (triggered, polled)\n   - Start timestamp\n   - End timestamp\n   - Status (pending, active, completed, failed, timeout)\n   - TLS session info (cipher suite, PSK identity)\n   - Error details (if failed)\n\n2. WHEN session progresses THEN the layer SHALL update status and timestamps\n3. WHEN listing sessions THEN the layer SHALL support filtering by device, card, status, date range\n4. WHEN querying sessions THEN the layer SHALL support pagination for large result sets\n5. WHEN session completes THEN the layer SHALL calculate and store duration\n6. WHEN analyzing sessions THEN the layer SHALL provide aggregation queries (success rate, avg duration)\n\n### Requirement 6: Communication Log Storage\n\n**User Story:** As a developer, I want all APDU exchanges to be logged in the database, so that I can analyze protocol behavior and debug issues.\n\n#### Acceptance Criteria\n\n1. WHEN logging communication THEN the layer SHALL store:\n   - Log entry ID\n   - Session ID (foreign key)\n   - Timestamp (millisecond precision)\n   - Direction (command/response)\n   - Raw data (hex)\n   - Decoded data (optional)\n   - Status word (for responses)\n   - Status message\n   - Latency (ms)\n\n2. WHEN logging THEN the layer SHALL batch inserts for performance\n3. WHEN querying logs THEN the layer SHALL support filtering by session, direction, status\n4. WHEN exporting logs THEN the layer SHALL support JSON and CSV formats\n5. WHEN purging logs THEN the layer SHALL support retention policy (delete logs older than X days)\n6. WHEN searching logs THEN the layer SHALL support hex pattern matching in raw data\n\n### Requirement 7: Test Result Storage\n\n**User Story:** As a QA engineer, I want test results stored in the database, so that I can track test history and generate compliance reports.\n\n#### Acceptance Criteria\n\n1. WHEN storing test result THEN the layer SHALL store:\n   - Test run ID (UUID)\n   - Test suite name\n   - Test case name\n   - Device ID\n   - Card ICCID\n   - Start/end timestamps\n   - Status (passed, failed, skipped, error)\n   - Duration (ms)\n   - Error message (if failed)\n   - Assertions (JSON array)\n   - Metadata (JSON)\n\n2. WHEN running test suite THEN the layer SHALL group results by run ID\n3. WHEN querying results THEN the layer SHALL support filtering by suite, status, date range\n4. WHEN generating report THEN the layer SHALL calculate pass/fail/skip counts\n5. WHEN comparing runs THEN the layer SHALL identify regressions (previously passed, now failed)\n6. WHEN exporting results THEN the layer SHALL support JUnit XML format for CI integration\n\n### Requirement 8: Server Settings Storage\n\n**User Story:** As an administrator, I want server settings stored in the database, so that configuration persists across restarts.\n\n#### Acceptance Criteria\n\n1. WHEN storing settings THEN the layer SHALL support:\n   - PSK-TLS server configuration (port, cipher suites, timeouts)\n   - Dashboard configuration (port, authentication)\n   - Default device settings\n   - Logging configuration\n   - Retention policies\n\n2. WHEN reading settings THEN the layer SHALL provide defaults for missing values\n3. WHEN updating settings THEN the layer SHALL validate before saving\n4. WHEN settings change THEN the layer SHALL emit event for live reload\n5. WHEN exporting settings THEN the layer SHALL support YAML format\n6. WHEN importing settings THEN the layer SHALL merge with existing configuration\n\n### Requirement 9: Repository Pattern Implementation\n\n**User Story:** As a developer, I want a clean repository interface for data access, so that business logic is decoupled from database details.\n\n#### Acceptance Criteria\n\n1. WHEN accessing data THEN the layer SHALL provide repository classes per entity\n2. WHEN querying THEN repositories SHALL support:\n   - `get_by_id(id)` - single entity by primary key\n   - `get_all()` - all entities with optional limit\n   - `find_by(**kwargs)` - query by attributes\n   - `create(entity)` - insert new entity\n   - `update(entity)` - update existing entity\n   - `delete(id)` - remove entity\n   - `exists(id)` - check existence\n\n3. WHEN filtering THEN repositories SHALL support complex queries via filter objects\n4. WHEN paginating THEN repositories SHALL return page info (total, page, per_page)\n5. WHEN using transactions THEN repositories SHALL support context manager pattern\n6. WHEN testing THEN repositories SHALL be mockable for unit tests\n\n### Requirement 10: Query Optimization\n\n**User Story:** As a developer, I want database queries to be optimized, so that the application performs well with large datasets.\n\n#### Acceptance Criteria\n\n1. WHEN defining tables THEN the layer SHALL create appropriate indexes\n2. WHEN querying THEN the layer SHALL use eager loading to prevent N+1 queries\n3. WHEN bulk inserting THEN the layer SHALL use batch operations\n4. WHEN counting THEN the layer SHALL use COUNT queries instead of fetching all\n5. WHEN using SQLite THEN the layer SHALL configure WAL mode for concurrent reads\n6. WHEN connection pool exhausted THEN the layer SHALL queue requests with timeout\n\n### Requirement 11: Data Export and Import\n\n**User Story:** As a tester, I want to export and import database contents, so that I can share configurations and migrate between environments.\n\n#### Acceptance Criteria\n\n1. WHEN exporting THEN the layer SHALL support:\n   - Full database export (all tables)\n   - Selective export (specific tables/entities)\n   - JSON format for portability\n   - YAML format for configuration\n\n2. WHEN importing THEN the layer SHALL:\n   - Validate data structure\n   - Handle conflicts (skip, overwrite, merge)\n   - Report import summary (created, updated, skipped)\n\n3. WHEN backing up THEN the layer SHALL support SQLite file copy\n4. WHEN restoring THEN the layer SHALL validate backup integrity\n\n### Requirement 12: CLI Integration\n\n**User Story:** As a developer, I want database operations available via CLI, so that I can manage the database without code.\n\n#### Acceptance Criteria\n\n1. WHEN running `cardlink-db init` THEN the CLI SHALL create database and tables\n2. WHEN running `cardlink-db migrate` THEN the CLI SHALL run pending migrations\n3. WHEN running `cardlink-db status` THEN the CLI SHALL show connection info and migration status\n4. WHEN running `cardlink-db export` THEN the CLI SHALL export data to file:\n   - `--format` for JSON/YAML\n   - `--tables` for selective export\n   - `--output` for file path\n5. WHEN running `cardlink-db import` THEN the CLI SHALL import data from file\n6. WHEN running `cardlink-db purge` THEN the CLI SHALL delete old data:\n   - `--older-than` for retention period\n   - `--tables` for selective purge (logs, sessions)\n7. WHEN running `cardlink-db stats` THEN the CLI SHALL show table row counts and sizes\n\n### Requirement 13: Event Emission for Integration\n\n**User Story:** As a dashboard developer, I want the database layer to emit events, so that I can update the UI when data changes.\n\n#### Acceptance Criteria\n\n1. WHEN device is created/updated/deleted THEN the layer SHALL emit device_changed event\n2. WHEN card profile is created/updated/deleted THEN the layer SHALL emit profile_changed event\n3. WHEN session is created/updated THEN the layer SHALL emit session_changed event\n4. WHEN settings are updated THEN the layer SHALL emit settings_changed event\n5. WHEN batch operation completes THEN the layer SHALL emit batch_completed event\n6. WHEN database error occurs THEN the layer SHALL emit database_error event\n\n## Non-Functional Requirements\n\n### Code Architecture and Modularity\n\n- **Repository Pattern**: Separate repository classes per entity for clean data access\n- **Unit of Work**: Transaction management through session context\n- **Dependency Injection**: Inject database session, not hardcoded connections\n- **Schema Definition**: SQLAlchemy declarative models with clear relationships\n\n### Performance\n\n- **Connection pooling**: Appropriate pool size per backend (SQLite: 1, MySQL/PostgreSQL: 5-20)\n- **Query latency**: Simple queries < 10ms, complex queries < 100ms\n- **Bulk operations**: Support batch insert of 1000+ log entries per second\n- **Index coverage**: All foreign keys and commonly queried columns indexed\n\n### Compatibility\n\n- **SQLite**: 3.35+ (JSON functions, WAL mode)\n- **MySQL**: 8.0+ (JSON type, window functions)\n- **PostgreSQL**: 12+ (JSON functions, partitioning)\n- **SQLAlchemy**: 2.0+ (async support, type annotations)\n- **Alembic**: Latest version for migrations\n\n### Reliability\n\n- **Transaction safety**: All multi-statement operations in transactions\n- **Connection recovery**: Automatic reconnection on transient failures\n- **Data integrity**: Foreign key constraints enforced\n- **Backup support**: SQLite file backup, SQL dump for others\n\n### Security\n\n- **Credential handling**: Database credentials from environment variables\n- **Sensitive data encryption**: PSK keys encrypted at rest\n- **SQL injection prevention**: Parameterized queries via ORM\n- **Access logging**: Optional query logging for audit\n",
  "fileStats": {
    "size": 14318,
    "lines": 320,
    "lastModified": "2025-11-25T11:48:03.049Z"
  },
  "comments": []
}